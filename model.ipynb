{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gpu available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    train_on_gpu = True\n",
    "    print(\"Gpu available\")\n",
    "else:\n",
    "    train_on_gpu = False\n",
    "    print(\"Gpu not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (fc1): Linear(in_features=51, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc4): Linear(in_features=16, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32,16)\n",
    "        self.fc4 = torch.nn.Linear(16, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.softmax(self.fc4(x))\n",
    "        return x\n",
    "    \n",
    "input_dim = 51\n",
    "output_dim  = 3\n",
    "batch_size = 64\n",
    "model = NN(input_dim, output_dim)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv(\"Final/train.csv\")\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomLoader(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.np_input_X = self.dataframe[['Score_1', 'X_pos_1', ' Y_pos_1', ' Score_2', 'X_pos_2', ' Y_pos_2',\n",
    "                                   ' Score_3', 'X_pos_3', ' Y_pos_3', ' Score_4', 'X_pos_4', ' Y_pos_4',\n",
    "                                   ' Score_5', 'X_pos_5', ' Y_pos_5', ' Score_6', 'X_pos_6', ' Y_pos_6',\n",
    "                                   ' Score_7', 'X_pos_7', ' Y_pos_7', ' Score_8', 'X_pos_8', ' Y_pos_8',\n",
    "                                   ' Score_9', 'X_pos_9', ' Y_pos_9', ' Score_10', 'X_pos_10', ' Y_pos_10',\n",
    "                                   ' Score_11', 'X_pos_11', ' Y_pos_11', ' Score_12', 'X_pos_12',\n",
    "                                   ' Y_pos_12', ' Score_13', 'X_pos_13', ' Y_pos_13', ' Score_14',\n",
    "                                   'X_pos_14', ' Y_pos_14', ' Score_15', 'X_pos_15', ' Y_pos_15',\n",
    "                                   ' Score_16', 'X_pos_16', ' Y_pos_16', ' Score_17', 'X_pos_17',\n",
    "                                   ' Y_pos_17']].to_numpy(dtype=np.float32)\n",
    "        \n",
    "        self.np_input_y = self.dataframe['mapped_output'].to_numpy()\n",
    "        \n",
    "\n",
    "        \n",
    "#         self.input_X = (self.np_input_X).ToTensor()\n",
    "#         self.input_y = torch.from_numpy(self.np_input_y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        self.input_X = torch.from_numpy(self.np_input_X)\n",
    "        self.input_y = torch.from_numpy(self.np_input_y)\n",
    "        \n",
    "        sample = {'input': self.input_X[idx], 'output': self.input_y[idx]}\n",
    "        return sample\n",
    "    \n",
    "trainset = CustomLoader(csv_file='Final/train.csv')\n",
    "testset = CustomLoader(csv_file='Final/test.csv')\n",
    "\n",
    "# TO Making sure data is loaded\n",
    "# for i in range(len(trainset)):\n",
    "#     sample = trainset[i]\n",
    "#     print(sample['input'], \"\\n\", sample['output'])\n",
    "#     break\n",
    "# type(sample['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.2\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)\n",
    "validloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=valid_sampler)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.237662 \tValidation Loss: 1.192720\n",
      "Validation loss decreased (inf --> 1.192720).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.898757 \tValidation Loss: 0.791906\n",
      "Validation loss decreased (1.192720 --> 0.791906).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.794928 \tValidation Loss: 0.778037\n",
      "Validation loss decreased (0.791906 --> 0.778037).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.776275 \tValidation Loss: 0.767338\n",
      "Validation loss decreased (0.778037 --> 0.767338).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.766881 \tValidation Loss: 0.760804\n",
      "Validation loss decreased (0.767338 --> 0.760804).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.762228 \tValidation Loss: 0.756618\n",
      "Validation loss decreased (0.760804 --> 0.756618).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.756975 \tValidation Loss: 0.734380\n",
      "Validation loss decreased (0.756618 --> 0.734380).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.704469 \tValidation Loss: 0.636046\n",
      "Validation loss decreased (0.734380 --> 0.636046).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.617094 \tValidation Loss: 0.589555\n",
      "Validation loss decreased (0.636046 --> 0.589555).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.580518 \tValidation Loss: 0.574005\n",
      "Validation loss decreased (0.589555 --> 0.574005).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.569492 \tValidation Loss: 0.571294\n",
      "Validation loss decreased (0.574005 --> 0.571294).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.565295 \tValidation Loss: 0.565305\n",
      "Validation loss decreased (0.571294 --> 0.565305).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.561711 \tValidation Loss: 0.562270\n",
      "Validation loss decreased (0.565305 --> 0.562270).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.560006 \tValidation Loss: 0.560395\n",
      "Validation loss decreased (0.562270 --> 0.560395).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.557150 \tValidation Loss: 0.559168\n",
      "Validation loss decreased (0.560395 --> 0.559168).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.554984 \tValidation Loss: 0.558895\n",
      "Validation loss decreased (0.559168 --> 0.558895).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.554086 \tValidation Loss: 0.557510\n",
      "Validation loss decreased (0.558895 --> 0.557510).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.553806 \tValidation Loss: 0.557260\n",
      "Validation loss decreased (0.557510 --> 0.557260).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.553352 \tValidation Loss: 0.557010\n",
      "Validation loss decreased (0.557260 --> 0.557010).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.553045 \tValidation Loss: 0.559214\n",
      "Epoch: 21 \tTraining Loss: 0.553267 \tValidation Loss: 0.558026\n",
      "Epoch: 22 \tTraining Loss: 0.552796 \tValidation Loss: 0.556475\n",
      "Validation loss decreased (0.557010 --> 0.556475).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.552626 \tValidation Loss: 0.557859\n",
      "Epoch: 24 \tTraining Loss: 0.552550 \tValidation Loss: 0.556801\n",
      "Epoch: 25 \tTraining Loss: 0.552584 \tValidation Loss: 0.556903\n",
      "Epoch: 26 \tTraining Loss: 0.552280 \tValidation Loss: 0.555368\n",
      "Validation loss decreased (0.556475 --> 0.555368).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.552321 \tValidation Loss: 0.555286\n",
      "Validation loss decreased (0.555368 --> 0.555286).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.552119 \tValidation Loss: 0.557669\n",
      "Epoch: 29 \tTraining Loss: 0.552432 \tValidation Loss: 0.556523\n",
      "Epoch: 30 \tTraining Loss: 0.552166 \tValidation Loss: 0.555828\n",
      "Epoch: 31 \tTraining Loss: 0.551988 \tValidation Loss: 0.555697\n",
      "Epoch: 32 \tTraining Loss: 0.551914 \tValidation Loss: 0.554736\n",
      "Validation loss decreased (0.555286 --> 0.554736).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.551871 \tValidation Loss: 0.555653\n",
      "Epoch: 34 \tTraining Loss: 0.551857 \tValidation Loss: 0.555060\n",
      "Epoch: 35 \tTraining Loss: 0.551825 \tValidation Loss: 0.555004\n",
      "Epoch: 36 \tTraining Loss: 0.551783 \tValidation Loss: 0.554615\n",
      "Validation loss decreased (0.554736 --> 0.554615).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.551774 \tValidation Loss: 0.554604\n",
      "Validation loss decreased (0.554615 --> 0.554604).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.551744 \tValidation Loss: 0.555264\n",
      "Epoch: 39 \tTraining Loss: 0.551738 \tValidation Loss: 0.553793\n",
      "Validation loss decreased (0.554604 --> 0.553793).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.551740 \tValidation Loss: 0.555768\n",
      "Epoch: 41 \tTraining Loss: 0.551692 \tValidation Loss: 0.553774\n",
      "Validation loss decreased (0.553793 --> 0.553774).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.551682 \tValidation Loss: 0.555893\n",
      "Epoch: 43 \tTraining Loss: 0.551720 \tValidation Loss: 0.553352\n",
      "Validation loss decreased (0.553774 --> 0.553352).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.551711 \tValidation Loss: 0.554389\n",
      "Epoch: 45 \tTraining Loss: 0.551656 \tValidation Loss: 0.554452\n",
      "Epoch: 46 \tTraining Loss: 0.551637 \tValidation Loss: 0.554737\n",
      "Epoch: 47 \tTraining Loss: 0.551623 \tValidation Loss: 0.554155\n",
      "Epoch: 48 \tTraining Loss: 0.551625 \tValidation Loss: 0.553936\n",
      "Epoch: 49 \tTraining Loss: 0.551619 \tValidation Loss: 0.555243\n",
      "Epoch: 50 \tTraining Loss: 0.551606 \tValidation Loss: 0.554248\n",
      "Epoch: 51 \tTraining Loss: 0.551600 \tValidation Loss: 0.553876\n",
      "Epoch: 52 \tTraining Loss: 0.551587 \tValidation Loss: 0.554270\n",
      "Epoch: 53 \tTraining Loss: 0.551581 \tValidation Loss: 0.553851\n",
      "Epoch: 54 \tTraining Loss: 0.551576 \tValidation Loss: 0.554501\n",
      "Epoch: 55 \tTraining Loss: 0.551567 \tValidation Loss: 0.554036\n",
      "Epoch: 56 \tTraining Loss: 0.551565 \tValidation Loss: 0.553727\n",
      "Epoch: 57 \tTraining Loss: 0.551559 \tValidation Loss: 0.554016\n",
      "Epoch: 58 \tTraining Loss: 0.551554 \tValidation Loss: 0.553886\n",
      "Epoch: 59 \tTraining Loss: 0.551551 \tValidation Loss: 0.553971\n",
      "Epoch: 60 \tTraining Loss: 0.551547 \tValidation Loss: 0.554075\n",
      "Epoch: 61 \tTraining Loss: 0.551543 \tValidation Loss: 0.554097\n",
      "Epoch: 62 \tTraining Loss: 0.551540 \tValidation Loss: 0.553710\n",
      "Epoch: 63 \tTraining Loss: 0.551536 \tValidation Loss: 0.553954\n",
      "Epoch: 64 \tTraining Loss: 0.551531 \tValidation Loss: 0.553764\n",
      "Epoch: 65 \tTraining Loss: 0.551529 \tValidation Loss: 0.553651\n",
      "Epoch: 66 \tTraining Loss: 0.551527 \tValidation Loss: 0.554106\n",
      "Epoch: 67 \tTraining Loss: 0.551524 \tValidation Loss: 0.553730\n",
      "Epoch: 68 \tTraining Loss: 0.551521 \tValidation Loss: 0.553979\n",
      "Epoch: 69 \tTraining Loss: 0.551520 \tValidation Loss: 0.553476\n",
      "Epoch: 70 \tTraining Loss: 0.551516 \tValidation Loss: 0.554081\n",
      "Epoch: 71 \tTraining Loss: 0.551517 \tValidation Loss: 0.553386\n",
      "Epoch: 72 \tTraining Loss: 0.551520 \tValidation Loss: 0.554520\n",
      "Epoch: 73 \tTraining Loss: 0.551514 \tValidation Loss: 0.553697\n",
      "Epoch: 74 \tTraining Loss: 0.551508 \tValidation Loss: 0.553246\n",
      "Validation loss decreased (0.553352 --> 0.553246).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 0.551506 \tValidation Loss: 0.553397\n",
      "Epoch: 76 \tTraining Loss: 0.551504 \tValidation Loss: 0.553701\n",
      "Epoch: 77 \tTraining Loss: 0.551502 \tValidation Loss: 0.553392\n",
      "Epoch: 78 \tTraining Loss: 0.551505 \tValidation Loss: 0.553796\n",
      "Epoch: 79 \tTraining Loss: 0.551502 \tValidation Loss: 0.553657\n",
      "Epoch: 80 \tTraining Loss: 0.551497 \tValidation Loss: 0.553493\n",
      "Epoch: 81 \tTraining Loss: 0.551497 \tValidation Loss: 0.553489\n",
      "Epoch: 82 \tTraining Loss: 0.551495 \tValidation Loss: 0.553284\n",
      "Epoch: 83 \tTraining Loss: 0.551494 \tValidation Loss: 0.553541\n",
      "Epoch: 84 \tTraining Loss: 0.551491 \tValidation Loss: 0.553154\n",
      "Validation loss decreased (0.553246 --> 0.553154).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 0.551490 \tValidation Loss: 0.553713\n",
      "Epoch: 86 \tTraining Loss: 0.551490 \tValidation Loss: 0.553174\n",
      "Epoch: 87 \tTraining Loss: 0.551489 \tValidation Loss: 0.553264\n",
      "Epoch: 88 \tTraining Loss: 0.551489 \tValidation Loss: 0.553624\n",
      "Epoch: 89 \tTraining Loss: 0.551485 \tValidation Loss: 0.553048\n",
      "Validation loss decreased (0.553154 --> 0.553048).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 0.551485 \tValidation Loss: 0.553646\n",
      "Epoch: 91 \tTraining Loss: 0.551485 \tValidation Loss: 0.553036\n",
      "Validation loss decreased (0.553048 --> 0.553036).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 0.551482 \tValidation Loss: 0.553484\n",
      "Epoch: 93 \tTraining Loss: 0.551483 \tValidation Loss: 0.553074\n",
      "Epoch: 94 \tTraining Loss: 0.551483 \tValidation Loss: 0.553653\n",
      "Epoch: 95 \tTraining Loss: 0.551480 \tValidation Loss: 0.552735\n",
      "Validation loss decreased (0.553036 --> 0.552735).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 0.551480 \tValidation Loss: 0.553290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 \tTraining Loss: 0.551478 \tValidation Loss: 0.553148\n",
      "Epoch: 98 \tTraining Loss: 0.551478 \tValidation Loss: 0.553429\n",
      "Epoch: 99 \tTraining Loss: 0.551476 \tValidation Loss: 0.552917\n",
      "Epoch: 100 \tTraining Loss: 0.551477 \tValidation Loss: 0.553534\n",
      "Epoch: 101 \tTraining Loss: 0.551476 \tValidation Loss: 0.553244\n",
      "Epoch: 102 \tTraining Loss: 0.551475 \tValidation Loss: 0.553025\n",
      "Epoch: 103 \tTraining Loss: 0.551475 \tValidation Loss: 0.553258\n",
      "Epoch: 104 \tTraining Loss: 0.551474 \tValidation Loss: 0.553185\n",
      "Epoch: 105 \tTraining Loss: 0.551473 \tValidation Loss: 0.553325\n",
      "Epoch: 106 \tTraining Loss: 0.551472 \tValidation Loss: 0.552919\n",
      "Epoch: 107 \tTraining Loss: 0.551471 \tValidation Loss: 0.553211\n",
      "Epoch: 108 \tTraining Loss: 0.551471 \tValidation Loss: 0.553123\n",
      "Epoch: 109 \tTraining Loss: 0.551470 \tValidation Loss: 0.553125\n",
      "Epoch: 110 \tTraining Loss: 0.551470 \tValidation Loss: 0.553237\n",
      "Epoch: 111 \tTraining Loss: 0.551469 \tValidation Loss: 0.553038\n",
      "Epoch: 112 \tTraining Loss: 0.551469 \tValidation Loss: 0.553236\n",
      "Epoch: 113 \tTraining Loss: 0.551468 \tValidation Loss: 0.552931\n",
      "Epoch: 114 \tTraining Loss: 0.551468 \tValidation Loss: 0.553089\n",
      "Epoch: 115 \tTraining Loss: 0.551467 \tValidation Loss: 0.553128\n",
      "Epoch: 116 \tTraining Loss: 0.551467 \tValidation Loss: 0.552933\n",
      "Epoch: 117 \tTraining Loss: 0.551467 \tValidation Loss: 0.553238\n",
      "Epoch: 118 \tTraining Loss: 0.551466 \tValidation Loss: 0.552777\n",
      "Epoch: 119 \tTraining Loss: 0.551466 \tValidation Loss: 0.553014\n",
      "Epoch: 120 \tTraining Loss: 0.551465 \tValidation Loss: 0.553013\n",
      "Epoch: 121 \tTraining Loss: 0.551465 \tValidation Loss: 0.552906\n",
      "Epoch: 122 \tTraining Loss: 0.551465 \tValidation Loss: 0.553016\n",
      "Epoch: 123 \tTraining Loss: 0.551464 \tValidation Loss: 0.552902\n",
      "Epoch: 124 \tTraining Loss: 0.551464 \tValidation Loss: 0.552930\n",
      "Epoch: 125 \tTraining Loss: 0.551464 \tValidation Loss: 0.553097\n",
      "Epoch: 126 \tTraining Loss: 0.551463 \tValidation Loss: 0.552864\n",
      "Epoch: 127 \tTraining Loss: 0.551462 \tValidation Loss: 0.552857\n",
      "Epoch: 128 \tTraining Loss: 0.551462 \tValidation Loss: 0.553026\n",
      "Epoch: 129 \tTraining Loss: 0.551462 \tValidation Loss: 0.553106\n",
      "Epoch: 130 \tTraining Loss: 0.551462 \tValidation Loss: 0.552774\n",
      "Epoch: 131 \tTraining Loss: 0.551461 \tValidation Loss: 0.552776\n",
      "Epoch: 132 \tTraining Loss: 0.551461 \tValidation Loss: 0.553092\n",
      "Epoch: 133 \tTraining Loss: 0.551461 \tValidation Loss: 0.552864\n",
      "Epoch: 134 \tTraining Loss: 0.551461 \tValidation Loss: 0.552725\n",
      "Validation loss decreased (0.552735 --> 0.552725).  Saving model ...\n",
      "Epoch: 135 \tTraining Loss: 0.551461 \tValidation Loss: 0.553342\n",
      "Epoch: 136 \tTraining Loss: 0.551460 \tValidation Loss: 0.552777\n",
      "Epoch: 137 \tTraining Loss: 0.551460 \tValidation Loss: 0.552797\n",
      "Epoch: 138 \tTraining Loss: 0.551459 \tValidation Loss: 0.552909\n",
      "Epoch: 139 \tTraining Loss: 0.551459 \tValidation Loss: 0.552909\n",
      "Epoch: 140 \tTraining Loss: 0.551459 \tValidation Loss: 0.552771\n",
      "Epoch: 141 \tTraining Loss: 0.551459 \tValidation Loss: 0.552875\n",
      "Epoch: 142 \tTraining Loss: 0.551458 \tValidation Loss: 0.552816\n",
      "Epoch: 143 \tTraining Loss: 0.551458 \tValidation Loss: 0.552893\n",
      "Epoch: 144 \tTraining Loss: 0.551458 \tValidation Loss: 0.552898\n",
      "Epoch: 145 \tTraining Loss: 0.551458 \tValidation Loss: 0.552821\n",
      "Epoch: 146 \tTraining Loss: 0.551458 \tValidation Loss: 0.552592\n",
      "Validation loss decreased (0.552725 --> 0.552592).  Saving model ...\n",
      "Epoch: 147 \tTraining Loss: 0.551457 \tValidation Loss: 0.552806\n",
      "Epoch: 148 \tTraining Loss: 0.551457 \tValidation Loss: 0.552990\n",
      "Epoch: 149 \tTraining Loss: 0.551457 \tValidation Loss: 0.552654\n",
      "Epoch: 150 \tTraining Loss: 0.551457 \tValidation Loss: 0.553079\n",
      "Epoch: 151 \tTraining Loss: 0.551456 \tValidation Loss: 0.552482\n",
      "Validation loss decreased (0.552592 --> 0.552482).  Saving model ...\n",
      "Epoch: 152 \tTraining Loss: 0.551456 \tValidation Loss: 0.552640\n",
      "Epoch: 153 \tTraining Loss: 0.551456 \tValidation Loss: 0.553018\n",
      "Epoch: 154 \tTraining Loss: 0.551456 \tValidation Loss: 0.552719\n",
      "Epoch: 155 \tTraining Loss: 0.551456 \tValidation Loss: 0.552741\n",
      "Epoch: 156 \tTraining Loss: 0.551456 \tValidation Loss: 0.552704\n",
      "Epoch: 157 \tTraining Loss: 0.551455 \tValidation Loss: 0.552695\n",
      "Epoch: 158 \tTraining Loss: 0.551455 \tValidation Loss: 0.552914\n",
      "Epoch: 159 \tTraining Loss: 0.551455 \tValidation Loss: 0.552664\n",
      "Epoch: 160 \tTraining Loss: 0.551455 \tValidation Loss: 0.552648\n",
      "Epoch: 161 \tTraining Loss: 0.551455 \tValidation Loss: 0.552697\n",
      "Epoch: 162 \tTraining Loss: 0.551455 \tValidation Loss: 0.552935\n",
      "Epoch: 163 \tTraining Loss: 0.551454 \tValidation Loss: 0.552545\n",
      "Epoch: 164 \tTraining Loss: 0.551454 \tValidation Loss: 0.552666\n",
      "Epoch: 165 \tTraining Loss: 0.551454 \tValidation Loss: 0.552823\n",
      "Epoch: 166 \tTraining Loss: 0.551454 \tValidation Loss: 0.552721\n",
      "Epoch: 167 \tTraining Loss: 0.551454 \tValidation Loss: 0.552808\n",
      "Epoch: 168 \tTraining Loss: 0.551454 \tValidation Loss: 0.552776\n",
      "Epoch: 169 \tTraining Loss: 0.551454 \tValidation Loss: 0.552623\n",
      "Epoch: 170 \tTraining Loss: 0.551453 \tValidation Loss: 0.552773\n",
      "Epoch: 171 \tTraining Loss: 0.551453 \tValidation Loss: 0.552808\n",
      "Epoch: 172 \tTraining Loss: 0.551453 \tValidation Loss: 0.552682\n",
      "Epoch: 173 \tTraining Loss: 0.551453 \tValidation Loss: 0.552682\n",
      "Epoch: 174 \tTraining Loss: 0.551453 \tValidation Loss: 0.552726\n",
      "Epoch: 175 \tTraining Loss: 0.551453 \tValidation Loss: 0.552660\n",
      "Epoch: 176 \tTraining Loss: 0.551453 \tValidation Loss: 0.552663\n",
      "Epoch: 177 \tTraining Loss: 0.551453 \tValidation Loss: 0.552657\n",
      "Epoch: 178 \tTraining Loss: 0.551452 \tValidation Loss: 0.552841\n",
      "Epoch: 179 \tTraining Loss: 0.551452 \tValidation Loss: 0.552718\n",
      "Epoch: 180 \tTraining Loss: 0.551452 \tValidation Loss: 0.552589\n",
      "Epoch: 181 \tTraining Loss: 0.551452 \tValidation Loss: 0.552692\n",
      "Epoch: 182 \tTraining Loss: 0.551452 \tValidation Loss: 0.552595\n",
      "Epoch: 183 \tTraining Loss: 0.551452 \tValidation Loss: 0.552647\n",
      "Epoch: 184 \tTraining Loss: 0.551452 \tValidation Loss: 0.552772\n",
      "Epoch: 185 \tTraining Loss: 0.551452 \tValidation Loss: 0.552621\n",
      "Epoch: 186 \tTraining Loss: 0.551452 \tValidation Loss: 0.552726\n",
      "Epoch: 187 \tTraining Loss: 0.551452 \tValidation Loss: 0.552813\n",
      "Epoch: 188 \tTraining Loss: 0.551451 \tValidation Loss: 0.552432\n",
      "Validation loss decreased (0.552482 --> 0.552432).  Saving model ...\n",
      "Epoch: 189 \tTraining Loss: 0.551451 \tValidation Loss: 0.552760\n",
      "Epoch: 190 \tTraining Loss: 0.551451 \tValidation Loss: 0.552560\n",
      "Epoch: 191 \tTraining Loss: 0.551451 \tValidation Loss: 0.552816\n",
      "Epoch: 192 \tTraining Loss: 0.551451 \tValidation Loss: 0.552597\n",
      "Epoch: 193 \tTraining Loss: 0.551451 \tValidation Loss: 0.552667\n",
      "Epoch: 194 \tTraining Loss: 0.551451 \tValidation Loss: 0.552704\n",
      "Epoch: 195 \tTraining Loss: 0.551451 \tValidation Loss: 0.552623\n",
      "Epoch: 196 \tTraining Loss: 0.551451 \tValidation Loss: 0.552606\n",
      "Epoch: 197 \tTraining Loss: 0.551451 \tValidation Loss: 0.552734\n",
      "Epoch: 198 \tTraining Loss: 0.551451 \tValidation Loss: 0.552639\n",
      "Epoch: 199 \tTraining Loss: 0.551450 \tValidation Loss: 0.552624\n",
      "Epoch: 200 \tTraining Loss: 0.551450 \tValidation Loss: 0.552464\n",
      "Epoch: 201 \tTraining Loss: 0.551450 \tValidation Loss: 0.552791\n",
      "Epoch: 202 \tTraining Loss: 0.551450 \tValidation Loss: 0.552637\n",
      "Epoch: 203 \tTraining Loss: 0.551450 \tValidation Loss: 0.552549\n",
      "Epoch: 204 \tTraining Loss: 0.551450 \tValidation Loss: 0.552726\n",
      "Epoch: 205 \tTraining Loss: 0.551450 \tValidation Loss: 0.552566\n",
      "Epoch: 206 \tTraining Loss: 0.551450 \tValidation Loss: 0.552622\n",
      "Epoch: 207 \tTraining Loss: 0.551450 \tValidation Loss: 0.552592\n",
      "Epoch: 208 \tTraining Loss: 0.551450 \tValidation Loss: 0.552474\n",
      "Epoch: 209 \tTraining Loss: 0.551450 \tValidation Loss: 0.552734\n",
      "Epoch: 210 \tTraining Loss: 0.551450 \tValidation Loss: 0.552554\n",
      "Epoch: 211 \tTraining Loss: 0.551450 \tValidation Loss: 0.552409\n",
      "Validation loss decreased (0.552432 --> 0.552409).  Saving model ...\n",
      "Epoch: 212 \tTraining Loss: 0.551449 \tValidation Loss: 0.552611\n",
      "Epoch: 213 \tTraining Loss: 0.551449 \tValidation Loss: 0.552761\n",
      "Epoch: 214 \tTraining Loss: 0.551449 \tValidation Loss: 0.552546\n",
      "Epoch: 215 \tTraining Loss: 0.551449 \tValidation Loss: 0.552711\n",
      "Epoch: 216 \tTraining Loss: 0.551449 \tValidation Loss: 0.552511\n",
      "Epoch: 217 \tTraining Loss: 0.551449 \tValidation Loss: 0.552531\n",
      "Epoch: 218 \tTraining Loss: 0.551449 \tValidation Loss: 0.552637\n",
      "Epoch: 219 \tTraining Loss: 0.551449 \tValidation Loss: 0.552586\n",
      "Epoch: 220 \tTraining Loss: 0.551449 \tValidation Loss: 0.552553\n",
      "Epoch: 221 \tTraining Loss: 0.551449 \tValidation Loss: 0.552403\n",
      "Validation loss decreased (0.552409 --> 0.552403).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 222 \tTraining Loss: 0.551449 \tValidation Loss: 0.552681\n",
      "Epoch: 223 \tTraining Loss: 0.551449 \tValidation Loss: 0.552565\n",
      "Epoch: 224 \tTraining Loss: 0.551449 \tValidation Loss: 0.552584\n",
      "Epoch: 225 \tTraining Loss: 0.551449 \tValidation Loss: 0.552589\n",
      "Epoch: 226 \tTraining Loss: 0.551449 \tValidation Loss: 0.552539\n",
      "Epoch: 227 \tTraining Loss: 0.551449 \tValidation Loss: 0.552517\n",
      "Epoch: 228 \tTraining Loss: 0.551449 \tValidation Loss: 0.552478\n",
      "Epoch: 229 \tTraining Loss: 0.551449 \tValidation Loss: 0.552694\n",
      "Epoch: 230 \tTraining Loss: 0.551449 \tValidation Loss: 0.552435\n",
      "Epoch: 231 \tTraining Loss: 0.551448 \tValidation Loss: 0.552521\n",
      "Epoch: 232 \tTraining Loss: 0.551448 \tValidation Loss: 0.552580\n",
      "Epoch: 233 \tTraining Loss: 0.551448 \tValidation Loss: 0.552426\n",
      "Epoch: 234 \tTraining Loss: 0.551448 \tValidation Loss: 0.552672\n",
      "Epoch: 235 \tTraining Loss: 0.551448 \tValidation Loss: 0.552543\n",
      "Epoch: 236 \tTraining Loss: 0.551448 \tValidation Loss: 0.552472\n",
      "Epoch: 237 \tTraining Loss: 0.551448 \tValidation Loss: 0.552595\n",
      "Epoch: 238 \tTraining Loss: 0.551448 \tValidation Loss: 0.552505\n",
      "Epoch: 239 \tTraining Loss: 0.551448 \tValidation Loss: 0.552481\n",
      "Epoch: 240 \tTraining Loss: 0.551448 \tValidation Loss: 0.552622\n",
      "Epoch: 241 \tTraining Loss: 0.551448 \tValidation Loss: 0.552488\n",
      "Epoch: 242 \tTraining Loss: 0.551448 \tValidation Loss: 0.552560\n",
      "Epoch: 243 \tTraining Loss: 0.551448 \tValidation Loss: 0.552540\n",
      "Epoch: 244 \tTraining Loss: 0.551448 \tValidation Loss: 0.552498\n",
      "Epoch: 245 \tTraining Loss: 0.551448 \tValidation Loss: 0.552584\n",
      "Epoch: 246 \tTraining Loss: 0.551448 \tValidation Loss: 0.552684\n",
      "Epoch: 247 \tTraining Loss: 0.551448 \tValidation Loss: 0.552360\n",
      "Validation loss decreased (0.552403 --> 0.552360).  Saving model ...\n",
      "Epoch: 248 \tTraining Loss: 0.551448 \tValidation Loss: 0.552430\n",
      "Epoch: 249 \tTraining Loss: 0.551448 \tValidation Loss: 0.552472\n",
      "Epoch: 250 \tTraining Loss: 0.551448 \tValidation Loss: 0.552616\n",
      "Epoch: 251 \tTraining Loss: 0.551448 \tValidation Loss: 0.552532\n",
      "Epoch: 252 \tTraining Loss: 0.551448 \tValidation Loss: 0.552435\n",
      "Epoch: 253 \tTraining Loss: 0.551448 \tValidation Loss: 0.552422\n",
      "Epoch: 254 \tTraining Loss: 0.551448 \tValidation Loss: 0.552546\n",
      "Epoch: 255 \tTraining Loss: 0.551448 \tValidation Loss: 0.552494\n",
      "Epoch: 256 \tTraining Loss: 0.551447 \tValidation Loss: 0.552462\n",
      "Epoch: 257 \tTraining Loss: 0.551447 \tValidation Loss: 0.552430\n",
      "Epoch: 258 \tTraining Loss: 0.551447 \tValidation Loss: 0.552574\n",
      "Epoch: 259 \tTraining Loss: 0.551447 \tValidation Loss: 0.552532\n",
      "Epoch: 260 \tTraining Loss: 0.551447 \tValidation Loss: 0.552493\n",
      "Epoch: 261 \tTraining Loss: 0.551447 \tValidation Loss: 0.552469\n",
      "Epoch: 262 \tTraining Loss: 0.551447 \tValidation Loss: 0.552456\n",
      "Epoch: 263 \tTraining Loss: 0.551447 \tValidation Loss: 0.552425\n",
      "Epoch: 264 \tTraining Loss: 0.551447 \tValidation Loss: 0.552473\n",
      "Epoch: 265 \tTraining Loss: 0.551447 \tValidation Loss: 0.552438\n",
      "Epoch: 266 \tTraining Loss: 0.551447 \tValidation Loss: 0.552475\n",
      "Epoch: 267 \tTraining Loss: 0.551447 \tValidation Loss: 0.552372\n",
      "Epoch: 268 \tTraining Loss: 0.551447 \tValidation Loss: 0.552489\n",
      "Epoch: 269 \tTraining Loss: 0.551447 \tValidation Loss: 0.552540\n",
      "Epoch: 270 \tTraining Loss: 0.551447 \tValidation Loss: 0.552363\n",
      "Epoch: 271 \tTraining Loss: 0.551447 \tValidation Loss: 0.552483\n",
      "Epoch: 272 \tTraining Loss: 0.551447 \tValidation Loss: 0.552546\n",
      "Epoch: 273 \tTraining Loss: 0.551447 \tValidation Loss: 0.552467\n",
      "Epoch: 274 \tTraining Loss: 0.551447 \tValidation Loss: 0.552424\n",
      "Epoch: 275 \tTraining Loss: 0.551447 \tValidation Loss: 0.552390\n",
      "Epoch: 276 \tTraining Loss: 0.551447 \tValidation Loss: 0.552541\n",
      "Epoch: 277 \tTraining Loss: 0.551447 \tValidation Loss: 0.552407\n",
      "Epoch: 278 \tTraining Loss: 0.551447 \tValidation Loss: 0.552364\n",
      "Epoch: 279 \tTraining Loss: 0.551447 \tValidation Loss: 0.552449\n",
      "Epoch: 280 \tTraining Loss: 0.551447 \tValidation Loss: 0.552413\n",
      "Epoch: 281 \tTraining Loss: 0.551447 \tValidation Loss: 0.552419\n",
      "Epoch: 282 \tTraining Loss: 0.551447 \tValidation Loss: 0.552429\n",
      "Epoch: 283 \tTraining Loss: 0.551447 \tValidation Loss: 0.552529\n",
      "Epoch: 284 \tTraining Loss: 0.551447 \tValidation Loss: 0.552400\n",
      "Epoch: 285 \tTraining Loss: 0.551447 \tValidation Loss: 0.552364\n",
      "Epoch: 286 \tTraining Loss: 0.551447 \tValidation Loss: 0.552337\n",
      "Validation loss decreased (0.552360 --> 0.552337).  Saving model ...\n",
      "Epoch: 287 \tTraining Loss: 0.551447 \tValidation Loss: 0.552382\n",
      "Epoch: 288 \tTraining Loss: 0.551447 \tValidation Loss: 0.552445\n",
      "Epoch: 289 \tTraining Loss: 0.551447 \tValidation Loss: 0.552403\n",
      "Epoch: 290 \tTraining Loss: 0.551447 \tValidation Loss: 0.552400\n",
      "Epoch: 291 \tTraining Loss: 0.551447 \tValidation Loss: 0.552467\n",
      "Epoch: 292 \tTraining Loss: 0.551447 \tValidation Loss: 0.552320\n",
      "Validation loss decreased (0.552337 --> 0.552320).  Saving model ...\n",
      "Epoch: 293 \tTraining Loss: 0.551447 \tValidation Loss: 0.552268\n",
      "Validation loss decreased (0.552320 --> 0.552268).  Saving model ...\n",
      "Epoch: 294 \tTraining Loss: 0.551447 \tValidation Loss: 0.552473\n",
      "Epoch: 295 \tTraining Loss: 0.551447 \tValidation Loss: 0.552451\n",
      "Epoch: 296 \tTraining Loss: 0.551447 \tValidation Loss: 0.552271\n",
      "Epoch: 297 \tTraining Loss: 0.551447 \tValidation Loss: 0.552468\n",
      "Epoch: 298 \tTraining Loss: 0.551446 \tValidation Loss: 0.552314\n",
      "Epoch: 299 \tTraining Loss: 0.551446 \tValidation Loss: 0.552354\n",
      "Epoch: 300 \tTraining Loss: 0.551446 \tValidation Loss: 0.552292\n",
      "Epoch: 301 \tTraining Loss: 0.551446 \tValidation Loss: 0.552464\n",
      "Epoch: 302 \tTraining Loss: 0.551446 \tValidation Loss: 0.552328\n",
      "Epoch: 303 \tTraining Loss: 0.551446 \tValidation Loss: 0.552371\n",
      "Epoch: 304 \tTraining Loss: 0.551446 \tValidation Loss: 0.552426\n",
      "Epoch: 305 \tTraining Loss: 0.551446 \tValidation Loss: 0.552334\n",
      "Epoch: 306 \tTraining Loss: 0.551446 \tValidation Loss: 0.552310\n",
      "Epoch: 307 \tTraining Loss: 0.551446 \tValidation Loss: 0.552429\n",
      "Epoch: 308 \tTraining Loss: 0.551446 \tValidation Loss: 0.552290\n",
      "Epoch: 309 \tTraining Loss: 0.551446 \tValidation Loss: 0.552340\n",
      "Epoch: 310 \tTraining Loss: 0.551446 \tValidation Loss: 0.552315\n",
      "Epoch: 311 \tTraining Loss: 0.551446 \tValidation Loss: 0.552403\n",
      "Epoch: 312 \tTraining Loss: 0.551446 \tValidation Loss: 0.552321\n",
      "Epoch: 313 \tTraining Loss: 0.551446 \tValidation Loss: 0.552330\n",
      "Epoch: 314 \tTraining Loss: 0.551446 \tValidation Loss: 0.552322\n",
      "Epoch: 315 \tTraining Loss: 0.551446 \tValidation Loss: 0.552449\n",
      "Epoch: 316 \tTraining Loss: 0.551446 \tValidation Loss: 0.552358\n",
      "Epoch: 317 \tTraining Loss: 0.551446 \tValidation Loss: 0.552191\n",
      "Validation loss decreased (0.552268 --> 0.552191).  Saving model ...\n",
      "Epoch: 318 \tTraining Loss: 0.551446 \tValidation Loss: 0.552232\n",
      "Epoch: 319 \tTraining Loss: 0.551446 \tValidation Loss: 0.552337\n",
      "Epoch: 320 \tTraining Loss: 0.551446 \tValidation Loss: 0.552441\n",
      "Epoch: 321 \tTraining Loss: 0.551446 \tValidation Loss: 0.552281\n",
      "Epoch: 322 \tTraining Loss: 0.551446 \tValidation Loss: 0.552239\n",
      "Epoch: 323 \tTraining Loss: 0.551446 \tValidation Loss: 0.552299\n",
      "Epoch: 324 \tTraining Loss: 0.551446 \tValidation Loss: 0.552231\n",
      "Epoch: 325 \tTraining Loss: 0.551446 \tValidation Loss: 0.552289\n",
      "Epoch: 326 \tTraining Loss: 0.551446 \tValidation Loss: 0.552392\n",
      "Epoch: 327 \tTraining Loss: 0.551446 \tValidation Loss: 0.552234\n",
      "Epoch: 328 \tTraining Loss: 0.551446 \tValidation Loss: 0.552376\n",
      "Epoch: 329 \tTraining Loss: 0.551446 \tValidation Loss: 0.552287\n",
      "Epoch: 330 \tTraining Loss: 0.551446 \tValidation Loss: 0.552177\n",
      "Validation loss decreased (0.552191 --> 0.552177).  Saving model ...\n",
      "Epoch: 331 \tTraining Loss: 0.551446 \tValidation Loss: 0.552369\n",
      "Epoch: 332 \tTraining Loss: 0.551446 \tValidation Loss: 0.552303\n",
      "Epoch: 333 \tTraining Loss: 0.551446 \tValidation Loss: 0.552140\n",
      "Validation loss decreased (0.552177 --> 0.552140).  Saving model ...\n",
      "Epoch: 334 \tTraining Loss: 0.551446 \tValidation Loss: 0.552346\n",
      "Epoch: 335 \tTraining Loss: 0.551446 \tValidation Loss: 0.552238\n",
      "Epoch: 336 \tTraining Loss: 0.551446 \tValidation Loss: 0.552293\n",
      "Epoch: 337 \tTraining Loss: 0.551446 \tValidation Loss: 0.552267\n",
      "Epoch: 338 \tTraining Loss: 0.551446 \tValidation Loss: 0.552246\n",
      "Epoch: 339 \tTraining Loss: 0.551446 \tValidation Loss: 0.552283\n",
      "Epoch: 340 \tTraining Loss: 0.551446 \tValidation Loss: 0.552258\n",
      "Epoch: 341 \tTraining Loss: 0.551446 \tValidation Loss: 0.552266\n",
      "Epoch: 342 \tTraining Loss: 0.551446 \tValidation Loss: 0.552235\n",
      "Epoch: 343 \tTraining Loss: 0.551446 \tValidation Loss: 0.552183\n",
      "Epoch: 344 \tTraining Loss: 0.551446 \tValidation Loss: 0.552310\n",
      "Epoch: 345 \tTraining Loss: 0.551446 \tValidation Loss: 0.552356\n",
      "Epoch: 346 \tTraining Loss: 0.551446 \tValidation Loss: 0.552151\n",
      "Epoch: 347 \tTraining Loss: 0.551446 \tValidation Loss: 0.552346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 348 \tTraining Loss: 0.551446 \tValidation Loss: 0.552246\n",
      "Epoch: 349 \tTraining Loss: 0.551446 \tValidation Loss: 0.552220\n",
      "Epoch: 350 \tTraining Loss: 0.551446 \tValidation Loss: 0.552243\n",
      "Epoch: 351 \tTraining Loss: 0.551446 \tValidation Loss: 0.552178\n",
      "Epoch: 352 \tTraining Loss: 0.551446 \tValidation Loss: 0.552256\n",
      "Epoch: 353 \tTraining Loss: 0.551446 \tValidation Loss: 0.552185\n",
      "Epoch: 354 \tTraining Loss: 0.551446 \tValidation Loss: 0.552254\n",
      "Epoch: 355 \tTraining Loss: 0.551446 \tValidation Loss: 0.552221\n",
      "Epoch: 356 \tTraining Loss: 0.551446 \tValidation Loss: 0.552205\n",
      "Epoch: 357 \tTraining Loss: 0.551446 \tValidation Loss: 0.552273\n",
      "Epoch: 358 \tTraining Loss: 0.551446 \tValidation Loss: 0.552192\n",
      "Epoch: 359 \tTraining Loss: 0.551446 \tValidation Loss: 0.552237\n",
      "Epoch: 360 \tTraining Loss: 0.551446 \tValidation Loss: 0.552286\n",
      "Epoch: 361 \tTraining Loss: 0.551446 \tValidation Loss: 0.552227\n",
      "Epoch: 362 \tTraining Loss: 0.551446 \tValidation Loss: 0.552138\n",
      "Validation loss decreased (0.552140 --> 0.552138).  Saving model ...\n",
      "Epoch: 363 \tTraining Loss: 0.551446 \tValidation Loss: 0.552201\n",
      "Epoch: 364 \tTraining Loss: 0.551446 \tValidation Loss: 0.552206\n",
      "Epoch: 365 \tTraining Loss: 0.551446 \tValidation Loss: 0.552234\n",
      "Epoch: 366 \tTraining Loss: 0.551446 \tValidation Loss: 0.552248\n",
      "Epoch: 367 \tTraining Loss: 0.551446 \tValidation Loss: 0.552171\n",
      "Epoch: 368 \tTraining Loss: 0.551446 \tValidation Loss: 0.552143\n",
      "Epoch: 369 \tTraining Loss: 0.551446 \tValidation Loss: 0.552316\n",
      "Epoch: 370 \tTraining Loss: 0.551446 \tValidation Loss: 0.552114\n",
      "Validation loss decreased (0.552138 --> 0.552114).  Saving model ...\n",
      "Epoch: 371 \tTraining Loss: 0.551446 \tValidation Loss: 0.552144\n",
      "Epoch: 372 \tTraining Loss: 0.551446 \tValidation Loss: 0.552266\n",
      "Epoch: 373 \tTraining Loss: 0.551446 \tValidation Loss: 0.552196\n",
      "Epoch: 374 \tTraining Loss: 0.551446 \tValidation Loss: 0.552181\n",
      "Epoch: 375 \tTraining Loss: 0.551446 \tValidation Loss: 0.552202\n",
      "Epoch: 376 \tTraining Loss: 0.551446 \tValidation Loss: 0.552122\n",
      "Epoch: 377 \tTraining Loss: 0.551446 \tValidation Loss: 0.552293\n",
      "Epoch: 378 \tTraining Loss: 0.551446 \tValidation Loss: 0.552143\n",
      "Epoch: 379 \tTraining Loss: 0.551446 \tValidation Loss: 0.552175\n",
      "Epoch: 380 \tTraining Loss: 0.551445 \tValidation Loss: 0.552126\n",
      "Epoch: 381 \tTraining Loss: 0.551445 \tValidation Loss: 0.552154\n",
      "Epoch: 382 \tTraining Loss: 0.551446 \tValidation Loss: 0.552176\n",
      "Epoch: 383 \tTraining Loss: 0.551445 \tValidation Loss: 0.552169\n",
      "Epoch: 384 \tTraining Loss: 0.551445 \tValidation Loss: 0.552134\n",
      "Epoch: 385 \tTraining Loss: 0.551445 \tValidation Loss: 0.552231\n",
      "Epoch: 386 \tTraining Loss: 0.551445 \tValidation Loss: 0.552155\n",
      "Epoch: 387 \tTraining Loss: 0.551445 \tValidation Loss: 0.552172\n",
      "Epoch: 388 \tTraining Loss: 0.551445 \tValidation Loss: 0.552125\n",
      "Epoch: 389 \tTraining Loss: 0.551445 \tValidation Loss: 0.552100\n",
      "Validation loss decreased (0.552114 --> 0.552100).  Saving model ...\n",
      "Epoch: 390 \tTraining Loss: 0.551445 \tValidation Loss: 0.552207\n",
      "Epoch: 391 \tTraining Loss: 0.551445 \tValidation Loss: 0.552205\n",
      "Epoch: 392 \tTraining Loss: 0.551445 \tValidation Loss: 0.552130\n",
      "Epoch: 393 \tTraining Loss: 0.551445 \tValidation Loss: 0.552114\n",
      "Epoch: 394 \tTraining Loss: 0.551445 \tValidation Loss: 0.552135\n",
      "Epoch: 395 \tTraining Loss: 0.551445 \tValidation Loss: 0.552150\n",
      "Epoch: 396 \tTraining Loss: 0.551445 \tValidation Loss: 0.552121\n",
      "Epoch: 397 \tTraining Loss: 0.551445 \tValidation Loss: 0.552113\n",
      "Epoch: 398 \tTraining Loss: 0.551445 \tValidation Loss: 0.552096\n",
      "Validation loss decreased (0.552100 --> 0.552096).  Saving model ...\n",
      "Epoch: 399 \tTraining Loss: 0.551445 \tValidation Loss: 0.552106\n",
      "Epoch: 400 \tTraining Loss: 0.551445 \tValidation Loss: 0.552195\n",
      "Epoch: 401 \tTraining Loss: 0.551445 \tValidation Loss: 0.552101\n",
      "Epoch: 402 \tTraining Loss: 0.551445 \tValidation Loss: 0.552101\n",
      "Epoch: 403 \tTraining Loss: 0.551445 \tValidation Loss: 0.552130\n",
      "Epoch: 404 \tTraining Loss: 0.551445 \tValidation Loss: 0.552123\n",
      "Epoch: 405 \tTraining Loss: 0.551445 \tValidation Loss: 0.552086\n",
      "Validation loss decreased (0.552096 --> 0.552086).  Saving model ...\n",
      "Epoch: 406 \tTraining Loss: 0.551445 \tValidation Loss: 0.552111\n",
      "Epoch: 407 \tTraining Loss: 0.551445 \tValidation Loss: 0.552043\n",
      "Validation loss decreased (0.552086 --> 0.552043).  Saving model ...\n",
      "Epoch: 408 \tTraining Loss: 0.551445 \tValidation Loss: 0.552108\n",
      "Epoch: 409 \tTraining Loss: 0.551445 \tValidation Loss: 0.552109\n",
      "Epoch: 410 \tTraining Loss: 0.551445 \tValidation Loss: 0.552151\n",
      "Epoch: 411 \tTraining Loss: 0.551445 \tValidation Loss: 0.552007\n",
      "Validation loss decreased (0.552043 --> 0.552007).  Saving model ...\n",
      "Epoch: 412 \tTraining Loss: 0.551445 \tValidation Loss: 0.552162\n",
      "Epoch: 413 \tTraining Loss: 0.551445 \tValidation Loss: 0.552057\n",
      "Epoch: 414 \tTraining Loss: 0.551445 \tValidation Loss: 0.552137\n",
      "Epoch: 415 \tTraining Loss: 0.551445 \tValidation Loss: 0.552133\n",
      "Epoch: 416 \tTraining Loss: 0.551445 \tValidation Loss: 0.552057\n",
      "Epoch: 417 \tTraining Loss: 0.551445 \tValidation Loss: 0.552100\n",
      "Epoch: 418 \tTraining Loss: 0.551445 \tValidation Loss: 0.552090\n",
      "Epoch: 419 \tTraining Loss: 0.551445 \tValidation Loss: 0.552040\n",
      "Epoch: 420 \tTraining Loss: 0.551445 \tValidation Loss: 0.552057\n",
      "Epoch: 421 \tTraining Loss: 0.551445 \tValidation Loss: 0.552150\n",
      "Epoch: 422 \tTraining Loss: 0.551445 \tValidation Loss: 0.552087\n",
      "Epoch: 423 \tTraining Loss: 0.551445 \tValidation Loss: 0.552029\n",
      "Epoch: 424 \tTraining Loss: 0.551445 \tValidation Loss: 0.551991\n",
      "Validation loss decreased (0.552007 --> 0.551991).  Saving model ...\n",
      "Epoch: 425 \tTraining Loss: 0.551445 \tValidation Loss: 0.552113\n",
      "Epoch: 426 \tTraining Loss: 0.551445 \tValidation Loss: 0.552037\n",
      "Epoch: 427 \tTraining Loss: 0.551445 \tValidation Loss: 0.552086\n",
      "Epoch: 428 \tTraining Loss: 0.551445 \tValidation Loss: 0.552049\n",
      "Epoch: 429 \tTraining Loss: 0.551445 \tValidation Loss: 0.552049\n",
      "Epoch: 430 \tTraining Loss: 0.551445 \tValidation Loss: 0.552078\n",
      "Epoch: 431 \tTraining Loss: 0.551445 \tValidation Loss: 0.552128\n",
      "Epoch: 432 \tTraining Loss: 0.551445 \tValidation Loss: 0.552042\n",
      "Epoch: 433 \tTraining Loss: 0.551445 \tValidation Loss: 0.551984\n",
      "Validation loss decreased (0.551991 --> 0.551984).  Saving model ...\n",
      "Epoch: 434 \tTraining Loss: 0.551445 \tValidation Loss: 0.552107\n",
      "Epoch: 435 \tTraining Loss: 0.551445 \tValidation Loss: 0.552023\n",
      "Epoch: 436 \tTraining Loss: 0.551445 \tValidation Loss: 0.552050\n",
      "Epoch: 437 \tTraining Loss: 0.551445 \tValidation Loss: 0.552099\n",
      "Epoch: 438 \tTraining Loss: 0.551445 \tValidation Loss: 0.551974\n",
      "Validation loss decreased (0.551984 --> 0.551974).  Saving model ...\n",
      "Epoch: 439 \tTraining Loss: 0.551445 \tValidation Loss: 0.552065\n",
      "Epoch: 440 \tTraining Loss: 0.551445 \tValidation Loss: 0.552109\n",
      "Epoch: 441 \tTraining Loss: 0.551445 \tValidation Loss: 0.552013\n",
      "Epoch: 442 \tTraining Loss: 0.551445 \tValidation Loss: 0.552013\n",
      "Epoch: 443 \tTraining Loss: 0.551445 \tValidation Loss: 0.551995\n",
      "Epoch: 444 \tTraining Loss: 0.551445 \tValidation Loss: 0.552020\n",
      "Epoch: 445 \tTraining Loss: 0.551445 \tValidation Loss: 0.552095\n",
      "Epoch: 446 \tTraining Loss: 0.551445 \tValidation Loss: 0.552049\n",
      "Epoch: 447 \tTraining Loss: 0.551445 \tValidation Loss: 0.552001\n",
      "Epoch: 448 \tTraining Loss: 0.551445 \tValidation Loss: 0.552016\n",
      "Epoch: 449 \tTraining Loss: 0.551445 \tValidation Loss: 0.551997\n",
      "Epoch: 450 \tTraining Loss: 0.551445 \tValidation Loss: 0.551970\n",
      "Validation loss decreased (0.551974 --> 0.551970).  Saving model ...\n",
      "Epoch: 451 \tTraining Loss: 0.551445 \tValidation Loss: 0.552031\n",
      "Epoch: 452 \tTraining Loss: 0.551445 \tValidation Loss: 0.552013\n",
      "Epoch: 453 \tTraining Loss: 0.551445 \tValidation Loss: 0.552014\n",
      "Epoch: 454 \tTraining Loss: 0.551445 \tValidation Loss: 0.552007\n",
      "Epoch: 455 \tTraining Loss: 0.551445 \tValidation Loss: 0.551974\n",
      "Epoch: 456 \tTraining Loss: 0.551445 \tValidation Loss: 0.552009\n",
      "Epoch: 457 \tTraining Loss: 0.551445 \tValidation Loss: 0.552019\n",
      "Epoch: 458 \tTraining Loss: 0.551445 \tValidation Loss: 0.551980\n",
      "Epoch: 459 \tTraining Loss: 0.551445 \tValidation Loss: 0.551960\n",
      "Validation loss decreased (0.551970 --> 0.551960).  Saving model ...\n",
      "Epoch: 460 \tTraining Loss: 0.551445 \tValidation Loss: 0.551992\n",
      "Epoch: 461 \tTraining Loss: 0.551445 \tValidation Loss: 0.552008\n",
      "Epoch: 462 \tTraining Loss: 0.551445 \tValidation Loss: 0.551981\n",
      "Epoch: 463 \tTraining Loss: 0.551445 \tValidation Loss: 0.551917\n",
      "Validation loss decreased (0.551960 --> 0.551917).  Saving model ...\n",
      "Epoch: 464 \tTraining Loss: 0.551445 \tValidation Loss: 0.552037\n",
      "Epoch: 465 \tTraining Loss: 0.551445 \tValidation Loss: 0.552004\n",
      "Epoch: 466 \tTraining Loss: 0.551445 \tValidation Loss: 0.551993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 467 \tTraining Loss: 0.551445 \tValidation Loss: 0.551970\n",
      "Epoch: 468 \tTraining Loss: 0.551445 \tValidation Loss: 0.551928\n",
      "Epoch: 469 \tTraining Loss: 0.551445 \tValidation Loss: 0.552014\n",
      "Epoch: 470 \tTraining Loss: 0.551445 \tValidation Loss: 0.551994\n",
      "Epoch: 471 \tTraining Loss: 0.551445 \tValidation Loss: 0.551964\n",
      "Epoch: 472 \tTraining Loss: 0.551445 \tValidation Loss: 0.551926\n",
      "Epoch: 473 \tTraining Loss: 0.551445 \tValidation Loss: 0.551966\n",
      "Epoch: 474 \tTraining Loss: 0.551445 \tValidation Loss: 0.551989\n",
      "Epoch: 475 \tTraining Loss: 0.551445 \tValidation Loss: 0.551978\n",
      "Epoch: 476 \tTraining Loss: 0.551445 \tValidation Loss: 0.551941\n",
      "Epoch: 477 \tTraining Loss: 0.551445 \tValidation Loss: 0.551948\n",
      "Epoch: 478 \tTraining Loss: 0.551445 \tValidation Loss: 0.551920\n",
      "Epoch: 479 \tTraining Loss: 0.551445 \tValidation Loss: 0.551919\n",
      "Epoch: 480 \tTraining Loss: 0.551445 \tValidation Loss: 0.552022\n",
      "Epoch: 481 \tTraining Loss: 0.551445 \tValidation Loss: 0.551900\n",
      "Validation loss decreased (0.551917 --> 0.551900).  Saving model ...\n",
      "Epoch: 482 \tTraining Loss: 0.551445 \tValidation Loss: 0.551991\n",
      "Epoch: 483 \tTraining Loss: 0.551445 \tValidation Loss: 0.551961\n",
      "Epoch: 484 \tTraining Loss: 0.551445 \tValidation Loss: 0.551983\n",
      "Epoch: 485 \tTraining Loss: 0.551445 \tValidation Loss: 0.551999\n",
      "Epoch: 486 \tTraining Loss: 0.551445 \tValidation Loss: 0.551940\n",
      "Epoch: 487 \tTraining Loss: 0.551445 \tValidation Loss: 0.551888\n",
      "Validation loss decreased (0.551900 --> 0.551888).  Saving model ...\n",
      "Epoch: 488 \tTraining Loss: 0.551445 \tValidation Loss: 0.551967\n",
      "Epoch: 489 \tTraining Loss: 0.551445 \tValidation Loss: 0.551921\n",
      "Epoch: 490 \tTraining Loss: 0.551445 \tValidation Loss: 0.551986\n",
      "Epoch: 491 \tTraining Loss: 0.551445 \tValidation Loss: 0.551980\n",
      "Epoch: 492 \tTraining Loss: 0.551445 \tValidation Loss: 0.551928\n",
      "Epoch: 493 \tTraining Loss: 0.551445 \tValidation Loss: 0.551947\n",
      "Epoch: 494 \tTraining Loss: 0.551445 \tValidation Loss: 0.551936\n",
      "Epoch: 495 \tTraining Loss: 0.551445 \tValidation Loss: 0.551952\n",
      "Epoch: 496 \tTraining Loss: 0.551445 \tValidation Loss: 0.551964\n",
      "Epoch: 497 \tTraining Loss: 0.551445 \tValidation Loss: 0.551918\n",
      "Epoch: 498 \tTraining Loss: 0.551445 \tValidation Loss: 0.551962\n",
      "Epoch: 499 \tTraining Loss: 0.551445 \tValidation Loss: 0.551978\n",
      "Epoch: 500 \tTraining Loss: 0.551445 \tValidation Loss: 0.551951\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 500\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "if train_on_gpu:\n",
    "    model = model.cuda()\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for content in trainloader:\n",
    "#         print(\"Training....\")\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data = content['input']\n",
    "        target = content['output']\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for content in validloader:\n",
    "#         print(\"Validating\")\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data = content['input']\n",
    "        target = content['output']\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(trainloader.sampler)\n",
    "    valid_loss = valid_loss/len(validloader.sampler)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'output/posenet.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (fc1): Linear(in_features=51, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc4): Linear(in_features=16, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('output/posenet.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 100.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "if train_on_gpu:\n",
    "    model = model.cuda()\n",
    "count_ = list()\n",
    "for content in validloader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    data = content['input']\n",
    "    target = content['output']\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    _, pred_labels = torch.max(output, 1)\n",
    "    for i in range(0, len(target)):\n",
    "        if target[i]==pred_labels[i]:\n",
    "            count_.append(1)\n",
    "        else:\n",
    "            count_.append(0)\n",
    "\n",
    "print(\"Accuracy is:\", round(count_.count(1)/len(count_)*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46, 51])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "# Load the ONNX file\n",
    "model = onnx.load('output/posenet.onnx')\n",
    "\n",
    "# Import the ONNX model to Tensorflow\n",
    "tf_rep = prepare(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
